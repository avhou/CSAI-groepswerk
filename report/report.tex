\documentclass[]{article}
\usepackage[backend=biber, style=numeric]{biblatex}
\usepackage{paralist}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{float}

\graphicspath{ {./figures/} }
\usepackage{geometry}
\geometry{a4paper, portrait}

\addbibresource{report.bib}
\newtheorem{researchquestion}{RQ}

%opening
\title{Capita Selecta in Artificial Intelligence - Assignment 3}
\author{Arne Lescrauwaet \small(852617312) \and Joachim Verschelde \small(852594432) \and Alexander Van Hecke \small(852631385) \and Farid Rasoolzadeh Baghmisheh \small(852104522)}

\begin{document}

\maketitle

\section{Introduction} \label{sec:introduction}
This report details the step taken by Arne Lescrauwaet (852617312), Joachim Verschelde (852594432), Alexander Van Hecke (852631385) and Farid Rasoolzadeh Baghmisheh (852104522) for the assignment of the 2023/2024 Capita Selecta in Artificial Intelligence course organised by the Open University (\cite{ou}).

For this assignment we expand on the experiments described in \cite{durability}.
In this paper, the authors investigate the use of an LLM model and a RAG setup \cite{rag} to asses company durability disclosures.
By examining published sustainability reports of carbon-intensive companies, they investigate whether climate transition measures are disclosed in these reports.
A set of 64 indicators (questions) was used to investigate measures already taken or yet to be taken.
For each question, relevant passages in the published sustainability reports were retrieved and passed as additional context to the GPT-4 model.
Their main conclusions were :

\begin{itemize}
    \item Companies more often disclose information about measures yet to be taken (``Talk'' measures) than about measures that were already taken (``Walk'' measures).
    \item The RAG setup was judged by human experts and found to be mostly positive.
    \item An automated tool as developed by the authors can be a help to human experts, but are no replacements for human experts.
\end{itemize}

In this research, we want to expand on \cite{durability} in the following ways :

\begin{itemize}
    \item We examine data published by the fast fashion industry instead of carbon-intensive companies.
    \item We evaluate and compare several freely available LLM models instead of using a commercial LLM model.
    \item We evaluate additional indicators provided by dr. ir. Clara Maathuis of Open University \cite{ou}.
\end{itemize}

This research is related to the \textbf{text} cluster of the course \cite{ou}.
The text cluster introduces a number of techniques to improve the performance of LLM models for certain tasks.
One of the techniques discussed is RAG, first introduced in \cite{rag}.
RAG adds context when querying LLM models, allowing the model to get a better semantic understanding of the question and to generate a more relevant answer.
The main strenghts of a RAG based setup are \textbf{i)} the ability to access up-to-date information (no cutoff), \textbf{ii)} the ability to add domain specific knowledge, \textbf{iii)} efficiency, as the parameteric memory does not need to contain all knowledge and no retraining or fine-tuning is necessary, \textbf{iv)} the ability to generate more specific, more diverse and more factual answers and \textbf{v)} the scalability of the approach.
The main disadvantages of a RAG based setup include \textbf{i)} the context limits of LLM models, which restricts how much additional information can be passed, \textbf{ii)} the performance of the retriever as the weakest link in the setup, \textbf{iii)} the additional latency and complexity introduced, \textbf{iv)} the need to keep the knowledge base up-to-date.
Evaluation of climate transition disclosures presents an excellent concrete use case of RAG.

\section{Goal} \label{sec:goal}

The goal of this assignment is to expand on the RAG setup introduced in \cite{durability}, by using different data, different LLM models and additional metrics.

Instead of focusing on carbon-intensive industry, we want to examine sustainability reports published by fast fashion companies.
This illustrates the ability of RAG based setups to easily incorporate domain specific knowledge or questions.
Section \ref{sec:data analysis} details the data used.

As described in \cite{durability}, the GPT-4 model performs well on the specific task of examining sustainability reports.
It is interesting to see how freely available models compare to commercial models.
Furthermore, we expand the initial set of 64 indicators by examining two additional scenarios.
In the first scenario \textit{(greenwashing detection)} we assess if a company shows potential signs of involvement or engagement with greenwashing methods and practices.
In the second scenario \textit{(greenwashing mitigation)} we provide relevant mitigation methods if the company is indeed engaging in greenwashing practices.
In section \ref{sec:methodology} we detail the models we evaluate and the experiments conducted.

Section \ref{sec:evaluation} details the evaluation of the results.
We do both a quantitative and a qualitative evaluation.
Since the LLM is asked to not only judge whether the question is true or false, but also to provide argumentation about the decision made, it is important to evaluate the argumentation.
Metrics such as coherence, consistency and relevance are important in this context.

Finally, we summarize the approach and provide conclusions in section \ref{sec:conclusions}.

\section{Data analysis} \label{sec:data analysis}

For this assignment, we investigate the 2023 sustainability reports of two fast-fashion companies (\textbf{H\&M} and \textbf{Zara}).
There reports are publicly available in PDF format.
Since we use the \texttt{llama\_index} python package to extract the text from the PDF files, no additional data processing or cleaning is necessary.
The \texttt{SentenceSplitter} class is used to extract coherent text blocks from PDFs.
In order to keep sentences together as much as possible, we use a \texttt{chunk\_size} of 350, and a \texttt{chunk\_overlap} of 50.
We retrieve the 8 most relevant chunks as additional context for each question.

\section{Methodology and Implementation} \label{sec:methodology}

\subsection{Methodology}

As described in section \ref{sec:goal}, in this research we want to \textbf{(i)} use sustainability reports published by the fast fashion industry instead of the carbon-intensive industry, \textbf{(ii)} evaluate several LLM models instead of relying on ChatGPT alone, \textbf{(iii)} evaluate two additional scenarios containing questions that are focused more on greenwashing and \textbf{(iv)} compute a number of metrics to measure things like coherence and consistency.

We selected 4 publicly available Ollama LLMs models : \texttt{llama2}, \texttt{llama3:instruct}, \texttt{mistral} and \texttt{phi3:14b-instruct}.
Custom prompts were written for each model to accommodate for the difference in reasoning skills and the capability to return structured outputs.

The 2023 sustainability reports of two fast-fashion companies (\textbf{H\&M} and \textbf{Zara}) were selected as dataset.
In addition to the 64 indicators identified in \cite{durability}, dr. ir. Clara Maathuis of Open University \cite{ou} provided 16 additional questions, grouped in two scenarios.
The first scenario consists of 10 questions and assesses if a company shows signs potential signs of involvement or engagement with greenwashing methods and practices.
The second scenario consists of 6 questions and tries to come up with greenwashing mitigation strategies when it is determined a company uses greenwashing methods and practices.
These are useful for stakeholders (such as investors, regulators, etc) related to the fast fashion company.

Because we use different data and different models, no ground truths or labels were available, neither for the original 64 indicators nor for the 16 additional scenario questions.
We considered two options : \textbf{(i)} establish ground truth ourselves and \textbf{(ii)} letting a sufficiently sophisticated LLM model establish ground truth.
Given that none of the team members possess domain expertise, we opted against the first approach.
Instead, we selected the second option, utilizing ChatGPT to determine the ground truth for all questions.
It is important to acknowledge that this decision implies we accept any inherent biases within ChatGPT and we adopt its judgments as the standard for evaluation.

For each of the investigated companies, we iterate over the selected LLM models, then iterate over the hyperparameters, select the correct prompt and send the 64 (original indicators) + 16 (additional scenarios) questions to the LLM model.
All answers are stored for further statistical analysis.
Finally, relevant metrics for each combination of company, LLM model and hyperparameter selection are calculcated.

\begin{itemize}
    \item TODO TODO welke hyperparameters @Arne (?)
    \item TODO TODO welke metrieken @Joachim
    \item TODO TODO welke versie van ChatGPT is er exact gebruikt voor ground truth @Farid
\end{itemize}


\subsection{Implementation}

We build on the implementation of \cite{durability}, available on Github (\cite{github-orig}).
The authors use \texttt{llama\_index}, a Python framework that hides the details of accessing different LLM models.
This allows for code that uses different LLM models with only minor changes.
Processing unstructured data such as PDF documents is available out of the box.
It also provides support for building RAG solutions, as a vector store (configured by a number of hyperparameters) and document retrieval are available.

We made several changes to the original code of \cite{durability}.

\textbf{First}, the authors used ChatGPT, which is a cloud hosted LLM model.
Since we wanted to evaluate the performance of some of the publicly available LLM models, we decided to host these models ourselves using \href{https://ollama.com/}{Ollama}.
\texttt{Ollama} provides a REST interface to query models, and integration with \texttt{llama\_index} is available.
It also allows to force LLM models to generate structured output in JSON format, which greatly helped evaluate performance of the different models.
\textbf{Second}, instead of using a single prompt for all LLM models, we created separate prompt per model.
This was needed as the models used have different reasoning capabilities and some need more directions to generate structured output.
As in the original paper, we created separate prompts to extract the general info, the year info and the answers to the questions.
\textbf{Third}, the code was modified to loop over all available datasets, models, and hyperparameter settings.
\textbf{Finally}, code to calculate performance statistics and metrics was added.
This was not present in \cite{durability} as their analysis was mostly qualitative.

Our implementation is available on github (\cite{github}).

\section{Evaluation and Results} \label{sec:evaluation}
\subsection{Base Questions}
This section provides an analysis of the results from our four models applied to the Zara and $H\&M$ reports, 
using the outcomes of GPT-4o as the ground truth due to the absence of human experts. 
The discussion is organized into four parts: first, we examine the issue of unanswered questions; 
next, we evaluate the decision-making processes; then, we review the cited source pages; and finally, 
we analyze the disclosed indicators.
\subsubsection{Zara}
\textbf{Missing Data:} An analysis of missing responses reveals that both the Llama3 and 
Mistral models failed to answer six questions each compared to the one from the GPT model. 
All models exhibited issues with citing source pages. 
The Llama3 model performed the best in this regard, with only six missing citations, 
while the Llama2 model failed to provide any source page citations. The Mistral model was similar to Llama3, 
with eight missing source page citations. In contrast, the Phi3 model demonstrated significant shortcomings, 
failing to cite source pages for 41 entries.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{./images/Missing_Values.png}
    \caption{Comparison of Missing Values Across Models}
    \label{fig:image_label}
\end{figure}

\textbf{Decision Metrics:} The analysis of decisions reveals a noteworthy pattern in the Llama2 model, 
which consistently responded "yes" to all questions. 
In contrast, the Mistral model demonstrated a nearly balanced response distribution, 
with approximately 50$\%$ of answers being "yes" and 50$\%$ being "no." 
The Llama3 and Phi3 models generated more realistic results, 
aligning more closely with the ground truth responses from the GPT model, 
though they still underrepresented "yes" responses. 
Notably, both models leaned toward providing a higher proportion of "no" responses compared to "yes." 
Among them, the Mistral model showed a slightly stronger preference for "no" responses than the Llama3 model.
\newline\newline
An analysis of the missing decision entries for the Llama3 and Mistral models reveals the following patterns: 
in the case of the Llama3 model, three missing decisions are from the "target" category, 
two from the "strategy" category, and one from the "tracking" category. \newline
For the Mistral model, both the "strategy" and "tracking" categories have two missing entries each,
while the "target" and "governance" categories have one missing entry each. Notably, 
both models failed to provide an answer for question 36 and both the GPT and Mistral model failed on question 11.
\newline\newline
\textbf{Source Page Metrics:} The majority of questions with missing source pages from the Llama3, Mistral, and Phi3 models 
fall within the "strategy" and "tracking" categories. 
Both the Llama3 and Mistral models fail to cite source pages for question 36. 
Additionally, Llama3 and Phi3 fail to provide source pages for questions 9, 19, 62, and 49, 
while Mistral and Phi3 fail to cite source pages for questions 34, 50, 37, and 63. Notably, 
there is no question for which all three models fail to provide a source page.
\newline\newline
\textbf{Disclosed indicators:} 
When comparing the "yes" and "no" counts of each model across various categories, 
Llama3 and Phi3 exhibit overall similarities. However, 
a closer examination of their counts within subcategories uncovers subtle differences, 
particularly in the distribution of "yes" responses, while "no" responses remain relatively 
consistent between the two models. A similar observation applies to the comparison between Llama3 and GPT.
\newline\newline

\textbf{Answer Similarity:} 
In this section, 
we compare the generated answers to the ground truth answers provided by the GPT-4o model using TFIDF and 
cosine similarity. The analysis shows that the Llama3 model achieves the highest count of questions where 
it is most similar to the ground truth, with a total of 23 questions. The second position is held by the phi3 
model with 16 questions, followed by llama2 with 13 questions, and mistral with 12 questions.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{./images/highest_count_zara.jpg}
    \caption{Most similar response count}
    \label{fig:image_label}
\end{figure}

Furthermore, Llama3 also ranks highest in average similarity, achieving $49\%$ average similarity. This is followed by phi3 at $47\%$, mistral at $45\%$, and finally, llama2 at $44\%$.

\begin{center}
    \begin{tabular}{||c c||} 
     \hline
     \textbf{Model} & \textbf{Average Simularity}  \\ [0.5ex] 
     \hline
     Llama3 & 0.489030 \\ 
     \hline
     Phi3 & 0.466919  \\
     \hline
     Mistral & 0.450250 \\ 
     \hline
     Llama2 & 0.435199  \\
     \hline
    \end{tabular}
\end{center}
    

\subsubsection{H$\&$M}
\textbf{Missing Data:} An analysis of the $H\&M$ missing responses shows that the Mistral model failed to answer 
five questions. Similar to the Zara report, 
all models struggled with citing source pages. 
The Llama3 model remained the best-performing in this aspect, with only three missing citations compared to the one missing citation from the GPT model, 
while the Llama2 model once again failed to provide any source page citations. 
The Mistral model performed worse compared to the Zara report, 
with seven missing citations this time. 
The Phi3 model also showed a slight decline, failing to cite source pages for 43 entries.
\newline\newline

\textbf{Decision Metrics:}
Once again, the Llama2 model exclusively answered "yes," 
while the Mistral model maintained an approximately 50\% "yes"-"no" response rate. 
The Llama3 and Phi3 models exhibited response ratios similar to those observed in the Zara report.
Both Llama3 and Phi3 seem to underperform on the amount of yes votes compared to the ground turth model ($\sim$ 10 vs 20).
As before, the "target" category accounted for the highest number of missing decisions.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{./images/hm_comparison_decision.jpg}
    \caption{Comparison of Decision Counts Across Models}
    \label{fig:image_label}
\end{figure}

\textbf{Source Page Metrics:} The majority of questions with missing source pages from the Llama3, Mistral, 
and Phi3 models once again fall within the "strategy" and 
"tracking" categories. This time, the Llama3 and Mistral models do not have any overlap in their 
missing source pages. Both Llama3 and Phi3 fail to cite sources for questions 26 and 32, 
while Mistral and Phi3 fail to cite sources for questions 1, 10, 35, and 37. 
Notably, there is still no question for which all three models failed to provide a source page.
\newline\newline

\textbf{Disclosed indicators:} When comparing the "yes"-"no" counts of each model across different categories, 
the counts for Llama3 and Phi3 once again appear to be quite similar overall. However, 
a closer examination within subcategories reveals more pronounced differences compared to the Zara report. 
Meanwhile, the Mistral model continues to maintain an approximately 50-50 response rate across all categories.
\newline\newline

\textbf{Answer Similarity:} 
The analysis shows that the Phi3 model achieves the highest count of questions where 
it is most similar to the ground truth for the $H\&M$ data, with a total of 22 questions. The second position is held by the Llama3 
model with 20 questions, followed by mistral with 15 questions, and Llama2 with 7 questions.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{./images/highest_count_hm.jpg}
    \caption{Most similar response count}
    \label{fig:image_label}
\end{figure}

Furthermore, 
Llama3  ranks highest in average similarity again, 
achieving $47\%$ average similarity. This is followed by phi3 at $46\%$, mistral at $45\%$, and finally, 
llama2 at $38\%$.

\begin{center}
    \begin{tabular}{||c c||} 
     \hline
     \textbf{Model} & \textbf{Average Simularity}  \\ [0.5ex] 
     \hline
     Llama3 & 0.467212 \\ 
     \hline
     Phi3 & 0.457428  \\
     \hline
     Mistral & 0.450698 \\
     \hline
     Llama2 & 0.383788  \\
     \hline
    \end{tabular}
\end{center}

\subsection{Scenario Questions}

\subsection{Conclusion}
The evaluation and results highlight notable trends and differences across the four models when applied to both the Zara and $H\&M$ datasets. 
Llama3 consistently emerges as the strongest performer in terms of answer similarity, achieving the highest average similarity scores and the highest count of questions most similar to the GPT-4o ground truth. 
Phi3 also demonstrates strong performance, often ranking second in both metrics, though its performance in citing source pages remains a challenge. Mistral provides balanced decision-making but lags in similarity scores and citation completeness. 
Llama2, while consistent in its "yes" responses, struggles significantly with citing source pages and maintaining realistic decision-making patterns.  

Across both datasets, the majority of missing source pages are concentrated in the "strategy" and "tracking" categories. 
Llama3 and Mistral show overlapping weaknesses in certain questions, such as question 36 in the Zara dataset, 
while no model consistently fails to provide a source page for all questions. These patterns underline the need for further refinement of the models, 
particularly in their ability to reference source material accurately.  

Overall, while Llama3 leads in performance, and Phi3 follows closely, 
both models show room for improvement in citation and response diversity. 
The findings underscore the importance of balancing accuracy, decision consistency, 
and source citation across all evaluated models.

\subsection{Ethical considerations}

In this section, we discuss some ethical considerations.
In the setup we used, we ask LLM models to judge whether questions are true or false, and to provide argumentation about the decision made.
In the argumentation, the LLM model must refer to the source chunks that were used to base the decision on.
Therefore, this solution is reasonably \textbf{transparant} and \textbf{explainable}, as it provides insight in how a decision was made by quoting the relevant information.
However, this does not necessarily imply \textbf{trustworthiness}.
The authors of \cite{durability} warned that the RAG solution could generate valuable output but should not be seen as a replacement for human experts.
Furthermore, as we are comparing the answers to ground truth values produced by ChatGPT, we can only have faith in the produced answers and metrics if we trust the ground truths to be accurate.
There is a degree of \textbf{input bias} in our experiments.
Companies have a vested interest in publishing optimistic and forward-looking sustainability reports.
In other words, the reports used to provide argumentation to the questions answered, are not necessarily objective.
Furthermore, since ChatGPT was used to establish ground truths for the questions, any and all biases present in that model would leak in the performance metrics.
Finally, \textbf{regulation} is important in this context as well.
On the one hand, there is already regulation requiring companies to publish sustainability reports.
On the other hand, more regulation could be necessary to assure the reports are trustworthy and legally binding.

\section{Conclusions and Discussion} \label{sec:conclusions}

\begin{quotation}
    summarize the approach followed in this project and draw conclusions based on the results obtained
    \begin{itemize}
        \item Conclusions based on the approach followed and results obtained
        \item Discuss the meaning and findings of this project in a broader context considering the cluster selected and aim of this course
        \item Discuss limitations and future directions based on the findings obtained
    \end{itemize}
\end{quotation}

\printbibliography

\end{document}