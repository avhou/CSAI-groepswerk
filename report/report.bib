@article{buck_n_nodate,
	title = {N -gram {Counts} and {Language} {Models} from the {Common} {Crawl}},
	abstract = {We contribute 5-gram counts and language models trained on the Common Crawl corpus, a collection over 9 billion web pages. This release improves upon the Google n-gram counts in two key ways: the inclusion of low-count entries and deduplication to reduce boilerplate. By preserving singletons, we were able to use Kneser-Ney smoothing to build large language models. This paper describes how the corpus was processed with emphasis on the problems that arise in working with data at this scale. Our unpruned Kneser-Ney English 5-gram language model, built on 975 billion deduplicated tokens, contains over 500 billion unique n-grams. We show gains of 0.5–1.4 BLEU by using large language models to translate into various languages.},
	language = {en},
	author = {Buck, Christian and Heaﬁeld, Kenneth and van Ooyen, Bas},
}

@article{chen_empirical_1999,
	title = {An empirical study of smoothing techniques for language modeling},
	volume = {13},
	issn = {0885-2308},
	url = {https://www.sciencedirect.com/science/article/pii/S0885230899901286},
	doi = {10.1006/csla.1999.0128},
	abstract = {We survey the most widely-used algorithms for smoothing models for language n -gram modeling. We then present an extensive empirical comparison of several of these smoothing techniques, including those described by Jelinek and Mercer (1980); Katz (1987); Bell, Cleary and Witten (1990); Ney, Essen and Kneser (1994), and Kneser and Ney (1995). We investigate how factors such as training data size, training corpus (e.g. Brown vs. Wall Street Journal), count cutoffs, and n -gram order (bigram vs. trigram) affect the relative performance of these methods, which is measured through the cross-entropy of test data. We find that these factors can significantly affect the relative performance of models, with the most significant factor being training data size. Since no previous comparisons have examined these factors systematically, this is the first thorough characterization of the relative performance of various algorithms. In addition, we introduce methodologies for analyzing smoothing algorithm efficacy in detail, and using these techniques we motivate a novel variation of Kneser–Ney smoothing that consistently outperforms all other algorithms evaluated. Finally, results showing that improved language model smoothing leads to improved speech recognition performance are presented.},
	number = {4},
	urldate = {2023-10-05},
	journal = {Computer Speech \& Language},
	author = {Chen, Stanley F. and Goodman, Joshua},
	month = oct,
	year = {1999},
	pages = {359--394},
}

@article{heafield_scalable_nodate,
	title = {Scalable {Modified} {Kneser}-{Ney} {Language} {Model} {Estimation}},
	abstract = {We present an efﬁcient algorithm to estimate large modiﬁed Kneser-Ney models including interpolation. Streaming and sorting enables the algorithm to scale to much larger models by using a ﬁxed amount of RAM and variable amount of disk. Using one machine with 140 GB RAM for 2.8 days, we built an unpruned model on 126 billion tokens. Machine translation experiments with this model show improvement of 0.8 BLEU point over constrained systems for the 2013 Workshop on Machine Translation task in three language pairs. Our algorithm is also faster for small models: we estimated a model on 302 million tokens using 7.7\% of the RAM and 14.0\% of the wall time taken by SRILM. The code is open source as part of KenLM.},
	language = {en},
	author = {Heafield, Kenneth and Pouzyrevsky, Ivan and Clark, Jonathan H and Koehn, Philipp},
}

@article{pauls_faster_nodate,
	title = {Faster and {Smaller} {N}-{Gram} {Language} {Models}},
	abstract = {N -gram language models are a major resource bottleneck in machine translation. In this paper, we present several language model implementations that are both highly compact and fast to query. Our fastest implementation is as fast as the widely used SRILM while requiring only 25\% of the storage. Our most compact representation can store all 4 billion n-grams and associated counts for the Google n-gram corpus in 23 bits per n-gram, the most compact lossless representation to date, and even more compact than recent lossy compression techniques. We also discuss techniques for improving query speed during decoding, including a simple but novel language model caching technique that improves the query speed of our language models (and SRILM) by up to 300\%.},
	language = {en},
	author = {Pauls, Adam and Klein, Dan},
}

@article{chen_survey_2000,
	title = {A survey of smoothing techniques for {ME} models},
	volume = {8},
	issn = {1558-2353},
	url = {https://ieeexplore.ieee.org/abstract/document/817452},
	doi = {10.1109/89.817452},
	abstract = {In certain contexts, maximum entropy (ME) modeling can be viewed as maximum likelihood (ML) training for exponential models, and like other ML methods is prone to overfitting of training data. Several smoothing methods for ME models have been proposed to address this problem, but previous results do not make it clear how these smoothing methods compare with smoothing methods for other types of related models. In this work, we survey previous work in ME smoothing and compare the performance of several of these algorithms with conventional techniques for smoothing n-gram language models. Because of the mature body of research in n-gram model smoothing and the close connection between ME and conventional n-gram models, this domain is well-suited to gauge the performance of ME smoothing methods. Over a large number of data sets, we find that fuzzy ME smoothing performs as well as or better than all other algorithms under consideration. We contrast this method with previous n-gram smoothing methods to explain its superior performance.},
	number = {1},
	urldate = {2023-10-05},
	journal = {IEEE Transactions on Speech and Audio Processing},
	author = {Chen, S.F. and Rosenfeld, R.},
	month = jan,
	year = {2000},
	note = {Conference Name: IEEE Transactions on Speech and Audio Processing},
	pages = {37--50},
}

@article{james_modified_nodate,
	title = {Modified {Kneser}-{Ney} {Smoothing} of n-gram {Models}},
	language = {en},
	author = {James, Frankie},
}

@inproceedings{moore_improved_2009,
	address = {Suntec, Singapore},
	title = {Improved smoothing for {N}-gram language models based on ordinary counts},
	url = {http://portal.acm.org/citation.cfm?doid=1667583.1667691},
	doi = {10.3115/1667583.1667691},
	abstract = {Kneser-Ney (1995) smoothing and its variants are generally recognized as having the best perplexity of any known method for estimating N-gram language models. Kneser-Ney smoothing, however, requires nonstandard N-gram counts for the lowerorder models used to smooth the highestorder model. For some applications, this makes Kneser-Ney smoothing inappropriate or inconvenient. In this paper, we introduce a new smoothing method based on ordinary counts that outperforms all of the previous ordinary-count methods we have tested, with the new method eliminating most of the gap between Kneser-Ney and those methods.},
	language = {en},
	urldate = {2023-10-05},
	booktitle = {Proceedings of the {ACL}-{IJCNLP} 2009 {Conference} {Short} {Papers} on - {ACL}-{IJCNLP} '09},
	publisher = {Association for Computational Linguistics},
	author = {Moore, Robert C. and Quirk, Chris},
	year = {2009},
	pages = {349},
}

@inproceedings{avasthi_processing_2021,
	address = {Singapore},
	series = {Lecture {Notes} in {Networks} and {Systems}},
	title = {Processing {Large} {Text} {Corpus} {Using} {N}-{Gram} {Language} {Modeling} and {Smoothing}},
	isbn = {9789811596896},
	doi = {10.1007/978-981-15-9689-6_3},
	abstract = {The prediction of next word, letter or phrase for the user, while she is typing, is a really valuable tool for improving user experience. The users are communicating, writing reviews and expressing their opinion on such platforms frequently and many times while moving. It has become necessary to provide the user with an application that can reduce typing effort and spelling errors when they have limited time. The text data is getting larger in size due to the extensive use of all kinds of social media platforms and so implementation of text prediction application is difficult considering the size of text data to be processed for language modeling. This research paper’s primary objective is processing large text corpus and implementing a probabilistic model like N-grams to predict the next word when the user provides input. In this exploratory research, n-gram models are discussed and evaluated using Good Turing Estimation, perplexity measure and type-to-token ratio.},
	language = {en},
	booktitle = {Proceedings of the {Second} {International} {Conference} on {Information} {Management} and {Machine} {Intelligence}},
	publisher = {Springer},
	author = {Avasthi, Sandhya and Chauhan, Ritu and Acharjya, Debi Prasanna},
	editor = {Goyal, Dinesh and Gupta, Amit Kumar and Piuri, Vincenzo and Ganzha, Maria and Paprzycki, Marcin},
	year = {2021},
	keywords = {N-gram model, Natural language processing, Text mining},
	pages = {21--32},
}

@article{chen_evaluation_2008,
	title = {Evaluation {Metrics} {For} {Language} {Models}},
	url = {https://kilthub.cmu.edu/articles/journal_contribution/Evaluation_Metrics_For_Language_Models/6605324/1},
	doi = {10.1184/R1/6605324.v1},
	abstract = {The most widely-used evaluation metric for language models for speech recognition is the perplexity of test data. While perplexities can be calculated efficiently and without access to a speech recognizer, they often do not correlate well with speech recognition word-error rates. In this research, we attempt to find a measure that like perplexity is easily calculated but which better predicts speech recognition performance. We investigate two approaches; first, we attempt to extend perplexity by using similar measures that utilize information about language models that perplexity ignores. Second, we attempt to imitate the word-error calculation without using a speech recognizer by artificially generating speech recognition lattices. To test our new metrics, we have built over thirty varied language models. We find that perplexity correlates with word-error rate remarkably well when only considering n-gram models trained on in-domain data. When considering other types of models, our novel metrics are superior to perplexity for predicting speech recognition performance. However, we conclude that none of these measures predict word-error rate sufficiently accurately to be effective tools for language model evaluation in speech recognition.},
	language = {en},
	urldate = {2023-10-05},
	author = {Chen, Stanley F. and Beeferman, Douglas and Rosenfeld, Roni},
	month = jan,
	year = {2008},
	note = {Publisher: Carnegie Mellon University},
}

@misc{chen_evaluating_2021,
	title = {Evaluating {Large} {Language} {Models} {Trained} on {Code}},
	url = {http://arxiv.org/abs/2107.03374},
	doi = {10.48550/arXiv.2107.03374},
	abstract = {We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8\% of the problems, while GPT-3 solves 0\% and GPT-J solves 11.4\%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2\% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.},
	urldate = {2023-10-05},
	publisher = {arXiv},
	author = {Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and Ray, Alex and Puri, Raul and Krueger, Gretchen and Petrov, Michael and Khlaaf, Heidy and Sastry, Girish and Mishkin, Pamela and Chan, Brooke and Gray, Scott and Ryder, Nick and Pavlov, Mikhail and Power, Alethea and Kaiser, Lukasz and Bavarian, Mohammad and Winter, Clemens and Tillet, Philippe and Such, Felipe Petroski and Cummings, Dave and Plappert, Matthias and Chantzis, Fotios and Barnes, Elizabeth and Herbert-Voss, Ariel and Guss, William Hebgen and Nichol, Alex and Paino, Alex and Tezak, Nikolas and Tang, Jie and Babuschkin, Igor and Balaji, Suchir and Jain, Shantanu and Saunders, William and Hesse, Christopher and Carr, Andrew N. and Leike, Jan and Achiam, Josh and Misra, Vedant and Morikawa, Evan and Radford, Alec and Knight, Matthew and Brundage, Miles and Murati, Mira and Mayer, Katie and Welinder, Peter and McGrew, Bob and Amodei, Dario and McCandlish, Sam and Sutskever, Ilya and Zaremba, Wojciech},
	month = jul,
	year = {2021},
	note = {arXiv:2107.03374 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@inproceedings{kneser_improved_1995,
	title = {Improved backing-off for {M}-gram language modeling},
	volume = {1},
	url = {https://ieeexplore.ieee.org/abstract/document/479394},
	doi = {10.1109/ICASSP.1995.479394},
	abstract = {In stochastic language modeling, backing-off is a widely used method to cope with the sparse data problem. In case of unseen events this method backs off to a less specific distribution. In this paper we propose to use distributions which are especially optimized for the task of backing-off. Two different theoretical derivations lead to distributions which are quite different from the probability distributions that are usually used for backing-off. Experiments show an improvement of about 10\% in terms of perplexity and 5\% in terms of word error rate.},
	urldate = {2023-10-05},
	booktitle = {1995 {International} {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}},
	author = {Kneser, R. and Ney, H.},
	month = may,
	year = {1995},
	note = {ISSN: 1520-6149},
	pages = {181--184 vol.1},
}

@inproceedings{bickel_predicting_2005,
	address = {Vancouver, British Columbia, Canada},
	title = {Predicting sentences using {N}-gram language models},
	url = {http://portal.acm.org/citation.cfm?doid=1220575.1220600},
	doi = {10.3115/1220575.1220600},
	abstract = {We explore the beneﬁt that users in several application areas can experience from a “tab-complete” editing assistance function. We develop an evaluation metric and adapt N -gram language models to the problem of predicting the subsequent words, given an initial text fragment. Using an instance-based method as baseline, we empirically study the predictability of call-center emails, personal emails, weather reports, and cooking recipes.},
	language = {en},
	urldate = {2023-10-05},
	booktitle = {Proceedings of the conference on {Human} {Language} {Technology} and {Empirical} {Methods} in {Natural} {Language} {Processing}  - {HLT} '05},
	publisher = {Association for Computational Linguistics},
	author = {Bickel, Steffen and Haider, Peter and Scheffer, Tobias},
	year = {2005},
	pages = {193--200},
}

@inproceedings{siivola_growing_2005,
	title = {Growing an n-gram language model},
	url = {https://www.isca-speech.org/archive/interspeech_2005/siivola05_interspeech.html},
	doi = {10.21437/Interspeech.2005-24},
	abstract = {Traditionally, when building an n-gram model, we decide the span of the model history, collect the relevant statistics and estimate the model. The model can be pruned down to a smaller size by manipulating the statistics or the estimated model. This paper shows how an n-gram model can be built by adding suitable sets of n-grams to a unigram model until desired complexity is reached. Very high order n-grams can be used in the model, since the need for handling the full unpruned model is eliminated by the proposed technique. We compare our growing method to entropy based pruning. In Finnish speech recognition tests, the models trained by the growing method outperform the entropy pruned models of similar size.},
	language = {en},
	urldate = {2023-10-05},
	booktitle = {Interspeech 2005},
	publisher = {ISCA},
	author = {Siivola, Vesa and Pellom, Bryan L.},
	month = sep,
	year = {2005},
	pages = {1309--1312},
}

@inproceedings{popel_perplexity_2010,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Perplexity of n-{Gram} and {Dependency} {Language} {Models}},
	isbn = {978-3-642-15760-8},
	doi = {10.1007/978-3-642-15760-8_23},
	abstract = {Language models (LMs) are essential components of many applications such as speech recognition or machine translation. LMs factorize the probability of a string of words into a product of P(wi{\textbar}hi), where hiis the context (history) of word wi. Most LMs use previous words as the context. The paper presents two alternative approaches: post-ngram LMs (which use following words as context) and dependency LMs (which exploit dependency structure of a sentence and can use e.g. the governing word as context). Dependency LMs could be useful whenever a topology of a dependency tree is available, but its lexical labels are unknown, e.g. in tree-to-tree machine translation. In comparison with baseline interpolated trigram LM both of the approaches achieve significantly lower perplexity for all seven tested languages (Arabic, Catalan, Czech, English, Hungarian, Italian, Turkish).},
	language = {en},
	booktitle = {Text, {Speech} and {Dialogue}},
	publisher = {Springer},
	author = {Popel, Martin and Mareček, David},
	editor = {Sojka, Petr and Horák, Aleš and Kopeček, Ivan and Pala, Karel},
	year = {2010},
	keywords = {Directed Acyclic Graph, Machine Translation, Preceding Word, Speech Recognition, Word Form},
	pages = {173--180},
}

@article{oboyle_weighted_1994,
	title = {A weighted average n-gram model of natural language},
	volume = {8},
	issn = {0885-2308},
	url = {https://www.sciencedirect.com/science/article/pii/S0885230884710175},
	doi = {10.1006/csla.1994.1017},
	abstract = {A new n-gram model of natural language designed to aid speech recognition is presented in which the probabilities are calculated as a weighted average of maximum likelihood probabilities obtained from a training corpus. This simple approach produces a model that can be constructed quickly and is easily adapted either by changing the weights or by changing the training corpus. The model is compared with two other models; the first is based on Turing-Good estimates and uses Katz's back-off approach. The second model is a deleted estimate model which combines different probability distributions in approximately optimal proportions. We introduce a new measure for language models based on their performance when predicting words removed randomly from samples of unseen text. The performance of all three models using both this new measure and the existing measure of perplexity have been compared. Results indicate that the performance of the new model is close to the performance of the deleted estimate model, while both are superior to the Turing-Good model.},
	number = {4},
	urldate = {2023-10-05},
	journal = {Computer Speech \& Language},
	author = {O'Boyle, P. and Owens, M. and Smith, F. J.},
	month = oct,
	year = {1994},
	pages = {337--349},
}

@inproceedings{zhang_kneser-ney_2014,
	address = {Baltimore, Maryland},
	title = {Kneser-{Ney} {Smoothing} on {Expected} {Counts}},
	url = {http://aclweb.org/anthology/P14-1072},
	doi = {10.3115/v1/P14-1072},
	abstract = {Widely used in speech and language processing, Kneser-Ney (KN) smoothing has consistently been shown to be one of the best-performing smoothing methods. However, KN smoothing assumes integer counts, limiting its potential uses—for example, inside Expectation-Maximization. In this paper, we propose a generalization of KN smoothing that operates on fractional counts, or, more precisely, on distributions over counts. We rederive all the steps of KN smoothing to operate on count distributions instead of integral counts, and apply it to two tasks where KN smoothing was not applicable before: one in language model adaptation, and the other in word alignment. In both cases, our method improves performance signiﬁcantly.},
	language = {en},
	urldate = {2023-10-05},
	booktitle = {Proceedings of the 52nd {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Zhang, Hui and Chiang, David},
	year = {2014},
	pages = {765--774},
}

@inproceedings{iyer_analyzing_1997,
	title = {Analyzing and predicting language model improvements},
	url = {https://ieeexplore.ieee.org/abstract/document/659013},
	doi = {10.1109/ASRU.1997.659013},
	abstract = {Statistical n-gram language models are traditionally developed using perplexity as a measure of goodness. However, perplexity often demonstrates a poor correlation with recognition improvements, mainly because it fails to account for the acoustic confusability between words and for search errors in a recognizer. In this paper, we study alternatives to perplexity for predicting language model performance, including other global features as well as a new approach that predicts, with a high correlation (0.96), performance differences associated with localized changes in language models, given a recognition system. Experiments focus on the problem of augmenting in-domain Switchboard text with out-of-domain text from the Wall Street Journal and broadcast news that differ in both style and content from the in-domain data.},
	urldate = {2023-10-05},
	booktitle = {1997 {IEEE} {Workshop} on {Automatic} {Speech} {Recognition} and {Understanding} {Proceedings}},
	author = {Iyer, R. and Ostendorf, M. and Meteer, M.},
	month = dec,
	year = {1997},
	pages = {254--261},
}

@inproceedings{sundermeyer_estimation_2011,
	title = {On the estimation of discount parameters for language model smoothing},
	url = {https://www.isca-speech.org/archive/interspeech_2011/sundermeyer11_interspeech.html},
	doi = {10.21437/Interspeech.2011-250},
	abstract = {The goal of statistical language modeling is to ﬁnd probability estimates for arbitrary word sequences. To obtain non-zero values, the probability distributions found in the training data need to be smoothed. In the widely-used Kneser-Ney family of smoothing algorithms, this is achieved by absolute discounting. The discount parameters can be computed directly using some approximation formulas minimizing the leaving-one-out log-likelihood of the training data.},
	language = {en},
	urldate = {2023-10-06},
	booktitle = {Interspeech 2011},
	publisher = {ISCA},
	author = {Sundermeyer, Martin and Schlüter, Ralf and Ney, Hermann},
	month = aug,
	year = {2011},
	pages = {1433--1436},
}

@misc{noauthor_evaluation_2019,
	title = {Evaluation {Metrics} for {Language} {Modeling}},
	url = {https://thegradient.pub/understanding-evaluation-metrics-for-language-models/},
	abstract = {On different metrics for evaluating language models, the relationships among them, mathematical and empirical bounds for those metrics, and suggested best practices with regards to how to report them.},
	language = {en},
	urldate = {2023-10-16},
	journal = {The Gradient},
	month = oct,
	year = {2019},
}

@article{chen_evaluation_2008-1,
	title = {Evaluation {Metrics} {For} {Language} {Models}},
	url = {https://kilthub.cmu.edu/articles/journal_contribution/Evaluation_Metrics_For_Language_Models/6605324/1},
	doi = {10.1184/R1/6605324.v1},
	abstract = {The most widely-used evaluation metric for language models for speech recognition is the perplexity of test data. While perplexities can be calculated efficiently and without access to a speech recognizer, they often do not correlate well with speech recognition word-error rates. In this research, we attempt to find a measure that like perplexity is easily calculated but which better predicts speech recognition performance. We investigate two approaches; first, we attempt to extend perplexity by using similar measures that utilize information about language models that perplexity ignores. Second, we attempt to imitate the word-error calculation without using a speech recognizer by artificially generating speech recognition lattices. To test our new metrics, we have built over thirty varied language models. We find that perplexity correlates with word-error rate remarkably well when only considering n-gram models trained on in-domain data. When considering other types of models, our novel metrics are superior to perplexity for predicting speech recognition performance. However, we conclude that none of these measures predict word-error rate sufficiently accurately to be effective tools for language model evaluation in speech recognition.},
	language = {en},
	urldate = {2023-10-16},
	author = {Chen, Stanley F. and Beeferman, Douglas and Rosenfeld, Roni},
	month = jan,
	year = {2008},
	note = {Publisher: Carnegie Mellon University},
}

@misc{nguyen_n-gram_2020,
	title = {N-gram language models},
	url = {https://medium.com/mti-technology/n-gram-language-models-b125b9b62e58},
	abstract = {Part 3: Optimize model interpolation},
	language = {en},
	urldate = {2023-10-17},
	journal = {MTI Technology},
	author = {Nguyen, Khanh},
	month = nov,
	year = {2020},
}

@inproceedings{lo_s2orc_2020,
	address = {Online},
	title = {{S2ORC}: {The} {Semantic} {Scholar} {Open} {Research} {Corpus}},
	shorttitle = {{S2ORC}},
	url = {https://www.aclweb.org/anthology/2020.acl-main.447},
	doi = {10.18653/v1/2020.acl-main.447},
	abstract = {We introduce S2ORC, a large corpus of 81.1M English-language academic papers spanning many academic disciplines. The corpus consists of rich metadata, paper abstracts, resolved bibliographic references, as well as structured full text for 8.1M open access papers. Full text is annotated with automatically-detected inline mentions of citations, figures, and tables, each linked to their corresponding paper objects. In S2ORC, we aggregate papers from hundreds of academic publishers and digital archives into a unified source, and create the largest publicly-available collection of machine-readable academic text to date. We hope this resource will facilitate research and development of tools and tasks for text mining over academic text.},
	language = {en},
	urldate = {2023-10-17},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Lo, Kyle and Wang, Lucy Lu and Neumann, Mark and Kinney, Rodney and Weld, Daniel},
	year = {2020},
	pages = {4969--4983},
}

@misc{stanford_nlp,
	title = {Speech and {Language} {Processing}},
	url = {https://web.stanford.edu/~jurafsky/slp3/},
	urldate = {2023-11-13},
	author = {Jurafsky, Dan and Martin, James H.},
}

@misc{datasets,
	title = {{S2ORC}: {The} {Semantic} {Scholar} {Open} {Research} {Corpus} {Dataset} — {Allen} {Institute} for {AI}},
	shorttitle = {{S2ORC}},
	url = {https://allenai.org/data/[id]?id=s2orc},
	abstract = {The largest collection of machine-readable academic papers to date for NLP \& text mining.},
	language = {en},
	urldate = {2023-11-13}
}

@article{miah_sentence_2022,
	title = {Sentence {Boundary} {Extraction} from {Scientific} {Literature} of {Electric} {Double} {Layer} {Capacitor} {Domain}: {Tools} and {Techniques}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2076-3417},
	shorttitle = {Sentence {Boundary} {Extraction} from {Scientific} {Literature} of {Electric} {Double} {Layer} {Capacitor} {Domain}},
	url = {https://www.mdpi.com/2076-3417/12/3/1352},
	doi = {10.3390/app12031352},
	abstract = {Given the growth of scientific literature on the web, particularly material science, acquiring data precisely from the literature has become more significant. Material information systems, or chemical information systems, play an essential role in discovering data, materials, or synthesis processes using the existing scientific literature. Processing and understanding the natural language of scientific literature is the backbone of these systems, which depend heavily on appropriate textual content. Appropriate textual content means a complete, meaningful sentence from a large chunk of textual content. The process of detecting the beginning and end of a sentence and extracting them as correct sentences is called sentence boundary extraction. The accurate extraction of sentence boundaries from PDF documents is essential for readability and natural language processing. Therefore, this study provides a comparative analysis of different tools for extracting PDF documents into text, which are available as Python libraries or packages and are widely used by the research community. The main objective is to find the most suitable technique among the available techniques that can correctly extract sentences from PDF files as text. The performance of the used techniques Pypdf2, Pdfminer.six, Pymupdf, Pdftotext, Tika, and Grobid is presented in terms of precision, recall, f-1 score, run time, and memory consumption. NLTK, Spacy, and Gensim Natural Language Processing (NLP) tools are used to identify sentence boundaries. Of all the techniques studied, the Grobid PDF extraction package using the NLP tool Spacy achieved the highest f-1 score of 93\% and consumed the least amount of memory at 46.13 MegaBytes.},
	language = {en},
	number = {3},
	urldate = {2023-11-13},
	journal = {Applied Sciences},
	author = {Miah, Md Saef Ullah and Sulaiman, Junaida and Sarwar, Talha Bin and Naseer, Ateeqa and Ashraf, Fasiha and Zamli, Kamal Zuhairi and Jose, Rajan},
	month = jan,
	year = {2022},
	note = {Number: 3
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {gensim, material informatics, material information system, Materials 4.0, NLP in material science, NLTK, PDF to text conversion, sentence boundary extraction, Spacy},
	pages = {1352},
}

@misc{ktai,
	title = {Key topics in Artificial Intelligence},
	shorttitle = {ktai},
	url = {https://brightspace.ou.nl/d2l/home/8790},
	language = {en},
  year = {2023},
	urldate = {2023-11-13}
}

@misc{youtube_nlp,
	title = {Nlp - 2.8 - Kneser-Ney Smoothing},
	shorttitle = {yt-nlp},
	url = {https://www.youtube.com/watch?v=ody1ysUTD7o},
	language = {en},
  year = {2023},
	urldate = {2023-11-13},
  author = { Miranda, André Ribeiro }
}


@misc{spacy,
	title = {spaCy},
	shorttitle = {spacy},
	url = {https://spacy.io},
	language = {en},
  year = {2023},
	urldate = {2023-11-13},
}
