
@inproceedings{rag,
	title = {Retrieval-{Augmented} {Generation} for {Knowledge}-{Intensive} {NLP} {Tasks}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html},
	abstract = {Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.},
	urldate = {2024-09-14},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and Küttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rocktäschel, Tim and Riedel, Sebastian and Kiela, Douwe},
	year = {2020},
	pages = {9459--9474},
	file = {Full Text PDF:/Users/alexander/Zotero/storage/UTIVKB4N/Lewis et al. - 2020 - Retrieval-Augmented Generation for Knowledge-Inten.pdf:application/pdf},
}

@misc{durability,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Combining {AI} and {Domain} {Expertise} to {Assess} {Corporate} {Climate} {Transition} {Disclosures}},
	url = {https://papers.ssrn.com/abstract=4826207},
	doi = {10.2139/ssrn.4826207},
	abstract = {Company transition plans toward a low-carbon economy are key for effective capital allocation and risk management. This paper proposes a set of 64 indicators to comprehensively assess transition plans and develops a Large Language Model-based tool to automate the assessment of company disclosures. We evaluate our tool with experts from 26 institutions, including financial regulators, investors, and non-governmental organizations. We apply the tool to the sustainability reports from carbon-intensive Climate Action 100+ companies. Our results show that companies tend to disclose more information related to target setting (talk), but fewer information related to the concrete implementation of strategies (walk). In addition, companies that disclose more information tend to have lower emissions. Our results highlight the need for increased scrutiny of companies' efforts and potential greenwashing risks. The complexity of transition activities presents a major challenge for comprehensive large-scale assessments. As shown in this paper, novel and flexible approaches using Large Language Models can serve as a remedy.},
	language = {en},
	urldate = {2025-01-11},
	publisher = {Social Science Research Network},
	author = {Colesanti Senni, Chiara and Schimanski, Tobias and Bingler, Julia and Ni, Jingwei and Leippold, Markus},
	month = may,
	year = {2024},
	keywords = {CA100+, Climate disclosure, human evaluation, Large Language Models, RAG system, transition plans},
	file = {Full Text PDF:/Users/alexander/Zotero/storage/VR4CZEUM/Colesanti Senni et al. - 2024 - Combining AI and Domain Expertise to Assess Corpor.pdf:application/pdf},
}

@misc{ou,
	title = {Startpagina - {IM1402}-{242512M} - {Capita} {Selecta} in {Artificial} {Intelligence}},
	url = {https://brightspace.ou.nl/d2l/home/17724},
	urldate = {2025-01-11},
	file = {Startpagina - IM1402-242512M - Capita Selecta in Artificial Intelligence:/Users/alexander/Zotero/storage/DSSG9WK2/17724.html:text/html},
}

@misc{github,
	title = {avhou/{CSAI}-groepswerk},
	url = {https://github.com/avhou/CSAI-groepswerk},
	urldate = {2025-01-25},
	author = {Hecke, Alexander Van},
	month = jan,
	year = {2025},
}

@misc{github-orig,
	title = {tobischimanski/transition\_NLP},
	url = {https://github.com/tobischimanski/transition_NLP},
	abstract = {This repo contains code and data to analyse the sustainability reports and transition plans of companies towards inconsistencies.},
	urldate = {2025-01-25},
	author = {Schimanski, Tobias},
	month = jan,
	year = {2025},
}
